{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip -q install \"transformers>=4.43\" \"accelerate>=0.33\" \"bitsandbytes>=0.43\" \"autoawq>=0.2.7\" torch --extra-index-url https://download.pytorch.org/whl/cu121\n",
    "!pip -q install fastapi uvicorn pyngrok "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "base_id = \"mistralai/Mistral-Nemo-Instruct-2407\"\n",
    "tok = AutoTokenizer.from_pretrained(base_id, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "bnb_cfg = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "model = AutoModelForCausalLM.from_pretrained(base_id, quantization_config=bnb_cfg, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "save_dir = \"mistral_nemo_instruct_2407_4bit_bnb\"\n",
    "try:\n",
    "    model.save_pretrained(save_dir, safe_serialization=True)\n",
    "    tok.save_pretrained(save_dir)\n",
    "    print(\"Saved quantized model to:\", save_dir)\n",
    "except Exception as e:\n",
    "    print(\"Save 4-bit not supported in this env:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "save_dir = \"/kaggle/working/mistral_nemo_instruct_2407_4bit_bnb\"\n",
    "\n",
    "def load_model_and_tokenizer(path_or_id):\n",
    "    bnb_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    tok = AutoTokenizer.from_pretrained(path_or_id, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        path_or_id,\n",
    "        quantization_config=bnb_cfg,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "    return tok, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if Path(save_dir).exists():\n",
    "    print(f\"Loading local 4-bit checkpoint: {save_dir}\")\n",
    "    tok, model = load_model_and_tokenizer(save_dir)\n",
    "else:\n",
    "    print(\"Local 4-bit checkpoint not found (saving likely not supported in this env).\")\n",
    "    print(f\"Falling back to: {fallback_hub_id}\")\n",
    "    tok, model = load_model_and_tokenizer(fallback_hub_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_new_tokens=300, temperature=0.7, top_p=0.95):\n",
    "    inputs = tok(prompt, return_tensors=\"pt\", max_length=3000)\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=1200,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        # do_sample=True,\n",
    "        pad_token_id=tok.eos_token_id\n",
    "    )\n",
    "    text = tok.decode(outputs[0], skip_special_tokens=True)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "NGROK_TOKEN = \"\"\n",
    "API_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request, HTTPException\n",
    "import uvicorn, threading, time, socket\n",
    "from pyngrok import ngrok, conf\n",
    "import nest_asyncio\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "async def gen(req: Request):\n",
    "    if req.headers.get(\"authorization\") != f\"Bearer {API_KEY}\":\n",
    "        raise HTTPException(status_code=401, detail=\"Unauthorized\")\n",
    "    data = await req.json()\n",
    "    return {\n",
    "        \"response\": generate_text(\n",
    "            data.get(\"prompt\", \"\")\n",
    "        )\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def free_port():\n",
    "    s = socket.socket()\n",
    "    s.bind(('', 0))\n",
    "    port = s.getsockname()[1]\n",
    "    s.close()\n",
    "    return port\n",
    "\n",
    "port = free_port()\n",
    "conf.get_default().auth_token = NGROK_TOKEN\n",
    "public_url = ngrok.connect(port).public_url\n",
    "print(\"Your public URL:\", public_url)\n",
    "\n",
    "# def run(): \n",
    "#     uvicorn.run(app, host=\"0.0.0.0\", port=port)\n",
    "\n",
    "def run():\n",
    "    import asyncio\n",
    "    from uvicorn import Config, Server\n",
    "\n",
    "    config = Config(app=app, host=\"0.0.0.0\", port=port, log_level=\"info\")\n",
    "    server = Server(config)\n",
    "\n",
    "    loop = asyncio.new_event_loop()\n",
    "    asyncio.set_event_loop(loop)\n",
    "    loop.run_until_complete(server.serve())\n",
    "\n",
    "\n",
    "# nest_asyncio.apply()\n",
    "threading.Thread(target=run, daemon=True).start()\n",
    "time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
